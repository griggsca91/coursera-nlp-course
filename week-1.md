# week 1

* tokenization -> splitting an input into useful information
* token
  * Result of a tokenization
  * represents a unit of information
  * many ways to tokenize, but most useful when combined with a set of rules
    * `nltk.tokenize.TreebankWordTokenizer`
  * https://github.com/hse-aml/natural-language-processing/blob/master/week1/lemmatization_demo.ipynb
  
* Stemming
  * Removes suffixes to the get the stem/root form of the word
* Lemmitization
  * 
